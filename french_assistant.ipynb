{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üá´üá∑ French Assistant - RAG-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞\n",
    "\n",
    "**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
    "1. Document Loader ‚Üí –∑–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π\n",
    "2. Text Splitter ‚Üí —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ chunks —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Å–µ–º–∞–Ω—Ç–∏–∫–∏\n",
    "3. Embeddings ‚Üí –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "4. Vector Store ‚Üí ChromaDB –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞\n",
    "5. Retriever ‚Üí MMR + Re-ranking –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "6. LLM Chain ‚Üí –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º\n",
    "7. Safety Layer ‚Üí —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q langchain langchain-community langchain-core langchain-text-splitters\n!pip install -q langchain-huggingface chromadb\n!pip install -q transformers sentence-transformers\n!pip install -q pyyaml accelerate bitsandbytes\n!pip install -q huggingface_hub"
  },
  {
   "cell_type": "markdown",
   "source": "## üîë 1.1 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HuggingFace —Ç–æ–∫–µ–Ω–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n\n–¢–æ–∫–µ–Ω –Ω–µ–æ–±—Ö–æ–¥–∏–º –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ gated –º–æ–¥–µ–ª—è–º (Llama, Saiga –∏ –¥—Ä.)\n\n–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω: https://huggingface.co/settings/tokens",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom getpass import getpass\n\n# –°–ø–æ—Å–æ–± 1: –í–≤–æ–¥ —Ç–æ–∫–µ–Ω–∞ –≤—Ä—É—á–Ω—É—é (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤–≤–æ–¥)\n# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å—Ç—Ä–æ–∫—É –Ω–∏–∂–µ –∏ –≤–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω\n# HF_TOKEN = getpass(\"üîë –í–≤–µ–¥–∏—Ç–µ HuggingFace —Ç–æ–∫–µ–Ω: \")\n\n# –°–ø–æ—Å–æ–± 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Colab Secrets (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Colab)\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    if HF_TOKEN:\n        print(\"‚úÖ –¢–æ–∫–µ–Ω –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ Colab Secrets\")\nexcept:\n    HF_TOKEN = None\n\n# –°–ø–æ—Å–æ–± 3: –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è\nif not HF_TOKEN:\n    HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGINGFACE_TOKEN\")\n    if HF_TOKEN:\n        print(\"‚úÖ –¢–æ–∫–µ–Ω –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è\")\n\n# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω –¥–ª—è huggingface_hub\nif HF_TOKEN:\n    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n    os.environ[\"HUGGINGFACE_TOKEN\"] = HF_TOKEN\n    print(f\"‚úÖ HuggingFace —Ç–æ–∫–µ–Ω —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–¥–ª–∏–Ω–∞: {len(HF_TOKEN)} —Å–∏–º–≤–æ–ª–æ–≤)\")\nelse:\n    print(\"‚ö†Ô∏è HuggingFace —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n    print(\"   –î–ª—è —Ä–∞–±–æ—Ç—ã —Å gated –º–æ–¥–µ–ª—è–º–∏ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ getpass –≤—ã—à–µ\")\n    print(\"   –∏–ª–∏ –¥–æ–±–∞–≤—å—Ç–µ —Ç–æ–∫–µ–Ω –≤ Colab Secrets (–∫–ª—é—á: HF_TOKEN)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ü§ñ 1.2 –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Saiga LLaMA3 8B\n\nSaiga ‚Äî —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –±–∞–∑–µ LLaMA3, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤.\n\n**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n- GPU —Å –º–∏–Ω–∏–º—É–º 16GB VRAM (T4, A100)\n- HuggingFace —Ç–æ–∫–µ–Ω (–º–æ–¥–µ–ª—å gated)\n\n**–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–µ–º 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU\nif not torch.cuda.is_available():\n    print(\"‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω! Saiga —Ç—Ä–µ–±—É–µ—Ç GPU –¥–ª—è —Ä–∞–±–æ—Ç—ã.\")\n    print(\"   –í Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n    SAIGA_MODEL = None\n    SAIGA_TOKENIZER = None\nelse:\n    print(f\"‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.get_device_name(0)}\")\n    print(f\"   –ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    \n    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n    MODEL_NAME = \"IlyaGusev/saiga_llama3_8b\"\n    \n    # 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True,\n    )\n    \n    print(f\"\\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {MODEL_NAME}...\")\n    print(\"   (–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)\")\n    \n    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n    SAIGA_TOKENIZER = AutoTokenizer.from_pretrained(\n        MODEL_NAME,\n        token=HF_TOKEN,\n        trust_remote_code=True\n    )\n    \n    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π\n    SAIGA_MODEL = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        token=HF_TOKEN,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n    )\n    \n    print(f\"‚úÖ –ú–æ–¥–µ–ª—å {MODEL_NAME} –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\")\n    print(f\"   –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –ø–∞–º—è—Ç—å GPU: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class SaigaLLM:\n    \"\"\"\n    –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ Saiga –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ RAG pipeline.\n    \n    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–æ–≤ Saiga (–Ω–∞ –±–∞–∑–µ ChatML).\n    \"\"\"\n    \n    def __init__(self, model, tokenizer, generation_config=None):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.generation_config = generation_config or {\n            'max_new_tokens': 1024,\n            'temperature': 0.3,\n            'top_p': 0.9,\n            'repetition_penalty': 1.1,\n            'do_sample': True,\n        }\n    \n    def _format_prompt(self, system_prompt: str, user_message: str) -> str:\n        \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Saiga/ChatML.\"\"\"\n        # Saiga –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç ChatML\n        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\"\"\"\n        return prompt\n    \n    def generate(self, system_prompt: str, user_message: str) -> str:\n        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∏ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\"\"\"\n        if self.model is None:\n            return \"[–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞]\"\n        \n        prompt = self._format_prompt(system_prompt, user_message)\n        \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=self.generation_config['max_new_tokens'],\n                temperature=self.generation_config['temperature'],\n                top_p=self.generation_config['top_p'],\n                repetition_penalty=self.generation_config['repetition_penalty'],\n                do_sample=self.generation_config['do_sample'],\n                pad_token_id=self.tokenizer.eos_token_id,\n            )\n        \n        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã\n        response = self.tokenizer.decode(\n            outputs[0][inputs['input_ids'].shape[1]:], \n            skip_special_tokens=True\n        )\n        \n        return response.strip()\n    \n    def predict(self, prompt: str) -> str:\n        \"\"\"–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å LangChain –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º.\"\"\"\n        # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ system/user\n        return self.generate(\"\", prompt)\n\n\n# –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä LLM –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\nif SAIGA_MODEL is not None and SAIGA_TOKENIZER is not None:\n    SAIGA_LLM = SaigaLLM(SAIGA_MODEL, SAIGA_TOKENIZER)\n    print(\"‚úÖ SaigaLLM –æ–±—ë—Ä—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∞!\")\nelse:\n    SAIGA_LLM = None\n    print(\"‚ö†Ô∏è SaigaLLM –Ω–µ —Å–æ–∑–¥–∞–Ω (–º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 2. –ò–º–ø–æ—Ä—Ç—ã –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport logging\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom collections import Counter\nimport hashlib\nimport yaml\n\n# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\nimport torch\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}\")\n\n# LangChain imports (updated for langchain >= 0.2)\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_core.prompts import PromptTemplate\n\n# HuggingFace Embeddings - —Å fallback –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\ntry:\n    from langchain_huggingface import HuggingFaceEmbeddings\nexcept ImportError:\n    from langchain_community.embeddings import HuggingFaceEmbeddings\n\n# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nprint(\"‚úÖ –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ YAML –∏–ª–∏ –∑–∞–¥–∞—Ç—å –∑–¥–µ—Å—å)\nCONFIG = {\n    'VECTOR_DB': {\n        'persist_directory': './data/chroma_db',\n        'collection_name': 'french_knowledge',\n        'embeddings': {\n            'model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n            'device': DEVICE  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GPU/CPU\n        }\n    },\n    'RAG_CONFIG': {\n        'chunking': {\n            'chunk_size': 500,\n            'chunk_overlap': 100,\n            'separators': [\"\\n\\n\", \"\\n\", \".\", \" \"]\n        },\n        'retrieval': {\n            'k': 5,\n            'fetch_k': 20,\n            'lambda_mult': 0.7,\n            'top_k_final': 3\n        }\n    },\n    'SAFETY': {\n        'input_filter': {\n            'max_length': 2000\n        }\n    },\n    'SYSTEM_PROMPT': '''–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ —Ä—É—Å—Å–∫–æ-—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ.\n–¢–≤–æ–∏ –∑–∞–¥–∞—á–∏:\n- –ü–æ–º–æ–≥–∞—Ç—å —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π\n- –û–±—ä—è—Å–Ω—è—Ç—å –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n- –†–∞–∑—ä—è—Å–Ω—è—Ç—å –∏–¥–∏–æ–º—ã –∏ —Ñ—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º—ã\n- –£–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ –∫–∞–∫ –∏—Ö –∏–∑–±–µ–∂–∞—Ç—å\n\n–í—Å–µ–≥–¥–∞ –¥–∞–≤–∞–π —Ç–æ—á–Ω—ã–µ, –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏.'''\n}\n\nprint(f\"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞! (device: {CONFIG['VECTOR_DB']['embeddings']['device']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 3. –£—Ç–∏–ª–∏—Ç—ã: Tracing –∏ Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TraceEvent:\n",
    "    \"\"\"–°–æ–±—ã—Ç–∏–µ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞.\"\"\"\n",
    "    timestamp: datetime\n",
    "    event_type: str\n",
    "    component: str\n",
    "    input_data: Any\n",
    "    output_data: Any\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "    duration_ms: float = 0.0\n",
    "\n",
    "\n",
    "class TracingManager:\n",
    "    \"\"\"–ú–µ–Ω–µ–¥–∂–µ—Ä —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤—Å–µ–≥–æ pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events: List[TraceEvent] = []\n",
    "        self.session_id = hashlib.md5(str(datetime.now()).encode()).hexdigest()[:8]\n",
    "    \n",
    "    def log_event(self, event_type: str, component: str, \n",
    "                  input_data: Any, output_data: Any, \n",
    "                  metadata: Dict = None, duration_ms: float = 0.0):\n",
    "        \"\"\"–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å–æ–±—ã—Ç–∏–µ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏.\"\"\"\n",
    "        event = TraceEvent(\n",
    "            timestamp=datetime.now(),\n",
    "            event_type=event_type,\n",
    "            component=component,\n",
    "            input_data=str(input_data)[:500],\n",
    "            output_data=str(output_data)[:500],\n",
    "            metadata=metadata or {},\n",
    "            duration_ms=duration_ms\n",
    "        )\n",
    "        self.events.append(event)\n",
    "        logger.debug(f\"[TRACE][{component}] {event_type}: {str(input_data)[:100]}...\")\n",
    "    \n",
    "    def get_trace_report(self) -> str:\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç –æ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–µ.\"\"\"\n",
    "        report = [f\"\\n{'='*60}\", f\"TRACE REPORT - Session: {self.session_id}\", f\"{'='*60}\"]\n",
    "        for event in self.events:\n",
    "            report.append(f\"\\n[{event.timestamp.strftime('%H:%M:%S.%f')[:-3]}] \"\n",
    "                         f\"{event.component} -> {event.event_type}\")\n",
    "            report.append(f\"  Input: {event.input_data[:100]}...\")\n",
    "            report.append(f\"  Output: {event.output_data[:100]}...\")\n",
    "            if event.duration_ms > 0:\n",
    "                report.append(f\"  Duration: {event.duration_ms:.2f}ms\")\n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "\n",
    "print(\"‚úÖ TracingManager –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafetyFilter:\n",
    "    \"\"\"\n",
    "    –§–∏–ª—å—Ç—Ä –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "    \n",
    "    –ó–∞—â–∏—Ç–∞ –æ—Ç:\n",
    "    1. Prompt injection\n",
    "    2. Off-topic –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "    3. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤—Ä–µ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n",
    "    4. –ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π (–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ retrieved –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)\n",
    "    \"\"\"\n",
    "    \n",
    "    INJECTION_PATTERNS = [\n",
    "        r\"(?i)ignore\\s+(all\\s+)?(previous|above|prior)\\s+(instructions?|prompts?)\",\n",
    "        r\"(?i)forget\\s+(everything|all|what)\",\n",
    "        r\"(?i)you\\s+are\\s+now\\s+a\",\n",
    "        r\"(?i)new\\s+instructions?:\",\n",
    "        r\"(?i)system\\s*prompt\",\n",
    "        r\"(?i)jailbreak\",\n",
    "        r\"(?i)bypass\\s+(the\\s+)?(filter|safety|security)\",\n",
    "        r\"(?i)act\\s+as\\s+(if|though)\",\n",
    "        r\"(?i)pretend\\s+(to\\s+be|you\\s+are)\",\n",
    "        r\"(?i)disregard\\s+(your|the)\\s+(rules|instructions)\",\n",
    "        r\"(?i)override\\s+(the\\s+)?(system|safety)\",\n",
    "    ]\n",
    "    \n",
    "    TOPIC_KEYWORDS_RU = [\n",
    "        '–ø–µ—Ä–µ–≤–æ–¥', '–ø–µ—Ä–µ–≤–µ–¥–∏', '—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫', '—Ñ—Ä–∞–Ω—Ü–∏—è', '–∫–∞–∫ —Å–∫–∞–∑–∞—Ç—å',\n",
    "        '–ø–æ-—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏', '–≥—Ä–∞–º–º–∞—Ç–∏–∫', '–∞—Ä—Ç–∏–∫–ª—å', '–≥–ª–∞–≥–æ–ª', '—Å–ª–æ–≤–æ',\n",
    "        '–≤—ã—Ä–∞–∂–µ–Ω–∏–µ', '—Ñ—Ä–∞–∑–∞', '–∏–¥–∏–æ–º', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏',\n",
    "        '—è–∑—ã–∫', '–ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏–µ', 'accent', '–≤—Ä–µ–º—è –≥–ª–∞–≥–æ–ª'\n",
    "    ]\n",
    "    \n",
    "    TOPIC_KEYWORDS_FR = [\n",
    "        'traduire', 'traduction', 'fran√ßais', 'russe', 'grammaire',\n",
    "        'verbe', 'article', 'expression', 'mot', 'phrase'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.max_length = config.get('max_length', 2000)\n",
    "        self.blocked_patterns = [re.compile(p) for p in self.INJECTION_PATTERNS]\n",
    "        logger.info(f\"SafetyFilter initialized with {len(self.INJECTION_PATTERNS)} injection patterns\")\n",
    "    \n",
    "    def check_injection(self, text: str) -> Tuple[bool, str]:\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ prompt injection.\"\"\"\n",
    "        for pattern in self.blocked_patterns:\n",
    "            if pattern.search(text):\n",
    "                return False, f\"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–æ–ø—ã—Ç–∫–∞ injection: {pattern.pattern}\"\n",
    "        return True, \"\"\n",
    "    \n",
    "    def check_topic_relevance(self, text: str) -> Tuple[bool, float]:\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ —Ç–µ–º–µ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        ru_matches = sum(1 for kw in self.TOPIC_KEYWORDS_RU if kw in text_lower)\n",
    "        fr_matches = sum(1 for kw in self.TOPIC_KEYWORDS_FR if kw in text_lower)\n",
    "        \n",
    "        score = (ru_matches + fr_matches) / min(10, len(text.split()))\n",
    "        has_french_chars = bool(re.search(r'[√©√®√™√´√†√¢√§√π√ª√º√¥√∂√Æ√Ø√ß]', text_lower))\n",
    "        \n",
    "        is_relevant = ru_matches > 0 or fr_matches > 0 or has_french_chars or score > 0.1\n",
    "        return is_relevant, min(score, 1.0)\n",
    "    \n",
    "    def check_length(self, text: str) -> Tuple[bool, str]:\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–ª–∏–Ω—É –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\"\"\"\n",
    "        if len(text) > self.max_length:\n",
    "            return False, f\"–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(text)} > {self.max_length})\"\n",
    "        return True, \"\"\n",
    "    \n",
    "    def filter_input(self, text: str) -> Tuple[bool, str, Dict]:\n",
    "        \"\"\"–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.\"\"\"\n",
    "        metadata = {'original_length': len(text), 'checks_passed': []}\n",
    "        \n",
    "        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã\n",
    "        is_ok, msg = self.check_length(text)\n",
    "        if not is_ok:\n",
    "            return False, msg, metadata\n",
    "        metadata['checks_passed'].append('length')\n",
    "        \n",
    "        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ injection\n",
    "        is_ok, msg = self.check_injection(text)\n",
    "        if not is_ok:\n",
    "            logger.warning(f\"Injection detected: {text[:100]}...\")\n",
    "            return False, \"–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –º–æ–≥—É –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å.\", metadata\n",
    "        metadata['checks_passed'].append('injection')\n",
    "        \n",
    "        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–º–µ\n",
    "        is_relevant, score = self.check_topic_relevance(text)\n",
    "        metadata['topic_score'] = score\n",
    "        \n",
    "        if not is_relevant:\n",
    "            return False, (\"–ò–∑–≤–∏–Ω–∏—Ç–µ, —è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–µ–≤–æ–¥–µ \"\n",
    "                          \"—Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π –∏ –≤–æ–ø—Ä–æ—Å–∞—Ö —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. \"\n",
    "                          \"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ.\"), metadata\n",
    "        metadata['checks_passed'].append('topic_relevance')\n",
    "        \n",
    "        return True, \"\", metadata\n",
    "    \n",
    "    def check_hallucination(self, response: str, context: str) -> Tuple[bool, float, List[str]]:\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏.\"\"\"\n",
    "        ungrounded_claims = []\n",
    "        \n",
    "        french_words_in_response = re.findall(\n",
    "            r'\\b[A-Za-z√©√®√™√´√†√¢√§√π√ª√º√¥√∂√Æ√Ø√ß]{3,}\\b', \n",
    "            response\n",
    "        )\n",
    "        \n",
    "        context_lower = context.lower()\n",
    "        grounded_count = 0\n",
    "        \n",
    "        for word in french_words_in_response:\n",
    "            if word.lower() in context_lower:\n",
    "                grounded_count += 1\n",
    "            elif len(word) > 5:\n",
    "                ungrounded_claims.append(word)\n",
    "        \n",
    "        total_words = len(french_words_in_response)\n",
    "        if total_words == 0:\n",
    "            return True, 1.0, []\n",
    "        \n",
    "        grounding_score = grounded_count / total_words\n",
    "        is_grounded = grounding_score > 0.3\n",
    "        \n",
    "        return is_grounded, grounding_score, ungrounded_claims[:5]\n",
    "\n",
    "\n",
    "print(\"‚úÖ SafetyFilter –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 4. Retrieval —Å–∏—Å—Ç–µ–º–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExpander:\n",
    "    \"\"\"\n",
    "    –†–∞—Å—à–∏—Ä–∏—Ç–µ–ª—å –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞.\n",
    "    \n",
    "    –¢–µ—Ö–Ω–∏–∫–∏:\n",
    "    1. –°–∏–Ω–æ–Ω–∏–º—ã –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏\n",
    "    2. HyDE (Hypothetical Document Embeddings)\n",
    "    3. Multi-query –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm=None):\n",
    "        self.llm = llm\n",
    "        self.synonyms = {\n",
    "            '–ø–µ—Ä–µ–≤–æ–¥': ['–ø–µ—Ä–µ–≤–µ—Å—Ç–∏', 'translation', 'traduire'],\n",
    "            '–≥–ª–∞–≥–æ–ª': ['verb', 'verbe', '—Å–ø—Ä—è–∂–µ–Ω–∏–µ'],\n",
    "            '–∞—Ä—Ç–∏–∫–ª—å': ['article', '–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π', '–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π'],\n",
    "            '–≤—Ä–µ–º—è': ['tense', 'temps', 'pr√©sent', 'pass√©', 'futur'],\n",
    "            '–∏–¥–∏–æ–º–∞': ['–≤—ã—Ä–∞–∂–µ–Ω–∏–µ', '—Ñ—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º', 'idiome', 'expression'],\n",
    "        }\n",
    "    \n",
    "    def expand_with_synonyms(self, query: str) -> List[str]:\n",
    "        \"\"\"–†–∞—Å—à–∏—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏.\"\"\"\n",
    "        expanded = [query]\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        for term, syns in self.synonyms.items():\n",
    "            if term in query_lower:\n",
    "                for syn in syns:\n",
    "                    expanded.append(query_lower.replace(term, syn))\n",
    "        \n",
    "        return list(set(expanded))[:4]\n",
    "    \n",
    "    def generate_hyde_document(self, query: str) -> str:\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è HyDE.\"\"\"\n",
    "        if self.llm:\n",
    "            prompt = f\"\"\"–ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∞–±–∑–∞—Ü, –∫–æ—Ç–æ—Ä—ã–π –±—ã –æ—Ç–≤–µ—á–∞–ª –Ω–∞ –≤–æ–ø—Ä–æ—Å:\n",
    "            \"{query}\"\n",
    "            –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∏ –∫–∞—Å–∞—Ç—å—Å—è —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–π –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞.\"\"\"\n",
    "            try:\n",
    "                return self.llm.predict(prompt)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"HyDE generation failed: {e}\")\n",
    "        \n",
    "        return f\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Ç–µ–º–µ: {query}. –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∏ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\"\n",
    "    \n",
    "    def expand_query(self, query: str, use_hyde: bool = False) -> List[str]:\n",
    "        \"\"\"–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞.\"\"\"\n",
    "        queries = self.expand_with_synonyms(query)\n",
    "        \n",
    "        if use_hyde:\n",
    "            hyde_doc = self.generate_hyde_document(query)\n",
    "            queries.append(hyde_doc)\n",
    "        \n",
    "        logger.debug(f\"Query expanded: {query} -> {len(queries)} variants\")\n",
    "        return queries\n",
    "\n",
    "\n",
    "print(\"‚úÖ QueryExpander –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRetriever:\n",
    "    \"\"\"\n",
    "    –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏:\n",
    "    1. Multi-query retrieval\n",
    "    2. MMR (Maximum Marginal Relevance)\n",
    "    3. Re-ranking —Å CrossEncoder\n",
    "    4. Contextual Compression\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore: Chroma, config: Dict, tracer: TracingManager):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.config = config\n",
    "        self.tracer = tracer\n",
    "        \n",
    "        self.base_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": config.get('k', 5),\n",
    "                \"fetch_k\": config.get('fetch_k', 20),\n",
    "                \"lambda_mult\": config.get('lambda_mult', 0.7)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.query_expander = QueryExpander()\n",
    "        logger.info(\"EnhancedRetriever initialized with MMR search\")\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Document]:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å\n",
    "        expanded_queries = self.query_expander.expand_query(query, use_hyde=True)\n",
    "        \n",
    "        self.tracer.log_event(\n",
    "            \"query_expansion\", \"EnhancedRetriever\",\n",
    "            query, f\"{len(expanded_queries)} variants\",\n",
    "            {\"variants\": expanded_queries}\n",
    "        )\n",
    "        \n",
    "        # 2. –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –∑–∞–ø—Ä–æ—Å–∞\n",
    "        all_docs = []\n",
    "        seen_contents = set()\n",
    "        \n",
    "        for q in expanded_queries:\n",
    "            docs = self.base_retriever.get_relevant_documents(q)\n",
    "            for doc in docs:\n",
    "                content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()\n",
    "                if content_hash not in seen_contents:\n",
    "                    seen_contents.add(content_hash)\n",
    "                    all_docs.append(doc)\n",
    "        \n",
    "        self.tracer.log_event(\n",
    "            \"multi_query_retrieval\", \"EnhancedRetriever\",\n",
    "            f\"{len(expanded_queries)} queries\", f\"{len(all_docs)} unique docs\"\n",
    "        )\n",
    "        \n",
    "        # 3. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "        scored_docs = []\n",
    "        query_terms = set(query.lower().split())\n",
    "        \n",
    "        for doc in all_docs:\n",
    "            content_lower = doc.page_content.lower()\n",
    "            score = sum(1 for term in query_terms if term in content_lower)\n",
    "            length_score = min(len(doc.page_content) / 500, 1.0)\n",
    "            final_score = score + length_score * 0.5\n",
    "            scored_docs.append((final_score, doc))\n",
    "        \n",
    "        scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # 4. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k\n",
    "        top_k = self.config.get('top_k_final', 3)\n",
    "        result_docs = [doc for _, doc in scored_docs[:top_k]]\n",
    "        \n",
    "        duration = (time.time() - start_time) * 1000\n",
    "        self.tracer.log_event(\n",
    "            \"retrieval_complete\", \"EnhancedRetriever\",\n",
    "            query, f\"{len(result_docs)} final docs\",\n",
    "            {\"scores\": [s for s, _ in scored_docs[:top_k]]},\n",
    "            duration\n",
    "        )\n",
    "        \n",
    "        return result_docs\n",
    "\n",
    "\n",
    "print(\"‚úÖ EnhancedRetriever –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 5. RAG Enhancements (Anti-Hallucination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalQuality(Enum):\n",
    "    \"\"\"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ retrieved –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\"\"\"\n",
    "    EXCELLENT = \"excellent\"\n",
    "    GOOD = \"good\"\n",
    "    PARTIAL = \"partial\"\n",
    "    POOR = \"poor\"\n",
    "    AMBIGUOUS = \"ambiguous\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VerificationResult:\n",
    "    \"\"\"–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.\"\"\"\n",
    "    is_verified: bool\n",
    "    confidence: float\n",
    "    issues: List[str]\n",
    "    corrections: List[str]\n",
    "    grounding_evidence: List[str]\n",
    "\n",
    "\n",
    "print(\"‚úÖ –ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è RAG Enhancements –≥–æ—Ç–æ–≤—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfVerification:\n",
    "    \"\"\"\n",
    "    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Chain-of-Verification (CoVe).\n",
    "    \n",
    "    –ü—Ä–æ—Ü–µ—Å—Å:\n",
    "    1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–Ω–æ–≤–∏–∫–∞ –æ—Ç–≤–µ—Ç–∞\n",
    "    2. –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤\n",
    "    3. –û—Ç–≤–µ—Ç—ã –Ω–∞ –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã\n",
    "    4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_base_content: str = \"\"):\n",
    "        self.kb_content = knowledge_base_content\n",
    "        self.verification_templates = {\n",
    "            'factual': \"–Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ '{claim}' —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤–µ—Ä–Ω—ã–º —Å–æ–≥–ª–∞—Å–Ω–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π?\",\n",
    "            'consistency': \"–°–æ–≥–ª–∞—Å—É–µ—Ç—Å—è –ª–∏ '{claim}' —Å –¥—Ä—É–≥–∏–º–∏ —á–∞—Å—Ç—è–º–∏ –æ—Ç–≤–µ—Ç–∞?\",\n",
    "            'completeness': \"–ü–æ–ª–Ω–æ—Å—Ç—å—é –ª–∏ —Ä–∞—Å–∫—Ä—ã—Ç –≤–æ–ø—Ä–æ—Å '{aspect}'?\",\n",
    "            'source': \"–ï—Å—Ç—å –ª–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–ª—è '{claim}'?\"\n",
    "        }\n",
    "    \n",
    "    def extract_claims(self, response: str) -> List[str]:\n",
    "        \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞.\"\"\"\n",
    "        claims = []\n",
    "        \n",
    "        translation_pattern = r'(?:–ø–µ—Ä–µ–≤–æ–¥|—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏|traduire)[:\\s]+([^.!?\\n]+)'\n",
    "        claims.extend(re.findall(translation_pattern, response, re.IGNORECASE))\n",
    "        \n",
    "        grammar_pattern = r'(?:–ø—Ä–∞–≤–∏–ª–æ|–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è|–æ–±—Ä–∞–∑—É–µ—Ç—Å—è|—Å–ø—Ä—è–≥–∞–µ—Ç—Å—è)[:\\s]+([^.!?\\n]+)'\n",
    "        claims.extend(re.findall(grammar_pattern, response, re.IGNORECASE))\n",
    "        \n",
    "        example_pattern = r'(?:–Ω–∞–ø—Ä–∏–º–µ—Ä|–ø—Ä–∏–º–µ—Ä)[:\\s]+([^.!?\\n]+)'\n",
    "        claims.extend(re.findall(example_pattern, response, re.IGNORECASE))\n",
    "        \n",
    "        return list(set(claims))[:10]\n",
    "    \n",
    "    def verify_claim(self, claim: str, context: str) -> Tuple[bool, float, str]:\n",
    "        \"\"\"–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ.\"\"\"\n",
    "        claim_lower = claim.lower()\n",
    "        context_lower = context.lower()\n",
    "        \n",
    "        claim_words = set(re.findall(r'\\b\\w{4,}\\b', claim_lower))\n",
    "        context_words = set(re.findall(r'\\b\\w{4,}\\b', context_lower))\n",
    "        \n",
    "        overlap = claim_words & context_words\n",
    "        \n",
    "        if len(claim_words) == 0:\n",
    "            return True, 1.0, \"Empty claim\"\n",
    "        \n",
    "        overlap_ratio = len(overlap) / len(claim_words)\n",
    "        \n",
    "        evidence = \"\"\n",
    "        for word in overlap:\n",
    "            sentences = re.split(r'[.!?]', context)\n",
    "            for sent in sentences:\n",
    "                if word in sent.lower():\n",
    "                    evidence = sent.strip()\n",
    "                    break\n",
    "        \n",
    "        is_verified = overlap_ratio > 0.3\n",
    "        confidence = min(overlap_ratio * 1.5, 1.0)\n",
    "        \n",
    "        return is_verified, confidence, evidence\n",
    "    \n",
    "    def run_verification(self, response: str, context: str) -> VerificationResult:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞.\"\"\"\n",
    "        claims = self.extract_claims(response)\n",
    "        \n",
    "        issues = []\n",
    "        corrections = []\n",
    "        grounding_evidence = []\n",
    "        \n",
    "        verified_count = 0\n",
    "        total_confidence = 0.0\n",
    "        \n",
    "        for claim in claims:\n",
    "            is_verified, confidence, evidence = self.verify_claim(claim, context)\n",
    "            total_confidence += confidence\n",
    "            \n",
    "            if is_verified:\n",
    "                verified_count += 1\n",
    "                if evidence:\n",
    "                    grounding_evidence.append(evidence[:100])\n",
    "            else:\n",
    "                issues.append(f\"–ù–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: {claim[:50]}...\")\n",
    "                corrections.append(f\"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å: {claim[:50]}...\")\n",
    "        \n",
    "        overall_verified = verified_count / max(len(claims), 1) > 0.5\n",
    "        avg_confidence = total_confidence / max(len(claims), 1)\n",
    "        \n",
    "        return VerificationResult(\n",
    "            is_verified=overall_verified,\n",
    "            confidence=avg_confidence,\n",
    "            issues=issues,\n",
    "            corrections=corrections,\n",
    "            grounding_evidence=grounding_evidence[:5]\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"‚úÖ ChainOfVerification –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfRAG:\n",
    "    \"\"\"\n",
    "    Self-RAG: —Å–∞–º–æ–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ retrieval –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "    \n",
    "    –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\n",
    "    1. Retrieval Token - –Ω—É–∂–µ–Ω –ª–∏ retrieval?\n",
    "    2. Relevance Token - —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç?\n",
    "    3. Support Token - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç–≤–µ—Ç?\n",
    "    4. Utility Token - –ø–æ–ª–µ–∑–µ–Ω –ª–∏ –æ—Ç–≤–µ—Ç?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.retrieval_triggers = {\n",
    "            'high': ['–ø–µ—Ä–µ–≤–æ–¥', '–∫–∞–∫ —Å–∫–∞–∑–∞—Ç—å', '–ø—Ä–∞–≤–∏–ª–æ', '–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞', '—Å–ø—Ä—è–∂–µ–Ω–∏–µ'],\n",
    "            'low': ['–ø—Ä–∏–≤–µ—Ç', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∫–∞', '–∫–∞–∫ –¥–µ–ª–∞']\n",
    "        }\n",
    "    \n",
    "    def assess_retrieval_need(self, query: str) -> Tuple[bool, float]:\n",
    "        \"\"\"–û—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω—É–∂–µ–Ω –ª–∏ retrieval –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        high_triggers = sum(1 for t in self.retrieval_triggers['high'] if t in query_lower)\n",
    "        low_triggers = sum(1 for t in self.retrieval_triggers['low'] if t in query_lower)\n",
    "        \n",
    "        if low_triggers > high_triggers:\n",
    "            return False, 0.9\n",
    "        elif high_triggers > 0:\n",
    "            return True, min(0.6 + high_triggers * 0.1, 1.0)\n",
    "        else:\n",
    "            return True, 0.7\n",
    "    \n",
    "    def assess_relevance(self, query: str, document: str) -> Tuple[RetrievalQuality, float]:\n",
    "        \"\"\"–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑–∞–ø—Ä–æ—Å—É.\"\"\"\n",
    "        query_words = set(query.lower().split())\n",
    "        doc_words = set(document.lower().split())\n",
    "        \n",
    "        intersection = len(query_words & doc_words)\n",
    "        union = len(query_words | doc_words)\n",
    "        jaccard = intersection / max(union, 1)\n",
    "        \n",
    "        key_terms_found = sum(1 for w in query_words if len(w) > 4 and w in document.lower())\n",
    "        \n",
    "        if jaccard > 0.3 and key_terms_found >= 2:\n",
    "            return RetrievalQuality.EXCELLENT, 0.9\n",
    "        elif jaccard > 0.2 or key_terms_found >= 1:\n",
    "            return RetrievalQuality.GOOD, 0.7\n",
    "        elif jaccard > 0.1:\n",
    "            return RetrievalQuality.PARTIAL, 0.5\n",
    "        else:\n",
    "            return RetrievalQuality.POOR, 0.3\n",
    "    \n",
    "    def assess_support(self, response: str, documents: List[str]) -> Tuple[bool, float]:\n",
    "        \"\"\"–û—Ü–µ–Ω–∏–≤–∞–µ—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.\"\"\"\n",
    "        combined_docs = \" \".join(documents).lower()\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        response_words = set(re.findall(r'\\b[a-z–∞-—è√©√®√™√´√†√¢√§√π√ª√º√¥√∂√Æ√Ø√ß]{4,}\\b', response_lower))\n",
    "        supported = sum(1 for w in response_words if w in combined_docs)\n",
    "        support_ratio = supported / max(len(response_words), 1)\n",
    "        \n",
    "        return support_ratio > 0.3, support_ratio\n",
    "    \n",
    "    def assess_utility(self, query: str, response: str) -> Tuple[bool, float]:\n",
    "        \"\"\"–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.\"\"\"\n",
    "        if len(response) < 50:\n",
    "            return False, 0.2\n",
    "        \n",
    "        has_structure = any(marker in response for marker in ['üìù', 'üí°', 'üìö', '‚ö†Ô∏è', '**'])\n",
    "        \n",
    "        query_topics = set(re.findall(r'\\b\\w{4,}\\b', query.lower()))\n",
    "        response_topics = set(re.findall(r'\\b\\w{4,}\\b', response.lower()))\n",
    "        topic_coverage = len(query_topics & response_topics) / max(len(query_topics), 1)\n",
    "        \n",
    "        utility_score = (0.3 if has_structure else 0) + topic_coverage * 0.7\n",
    "        return utility_score > 0.4, utility_score\n",
    "\n",
    "\n",
    "print(\"‚úÖ SelfRAG –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectiveRAG:\n",
    "    \"\"\"\n",
    "    CRAG - Corrective Retrieval Augmented Generation.\n",
    "    \n",
    "    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:\n",
    "    1. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ retrieved –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    2. –ï—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∏–∑–∫–æ–µ - –∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑:\n",
    "       - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫\n",
    "       - –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞\n",
    "       - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ fallback-–∑–Ω–∞–Ω–∏–π\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.self_rag = SelfRAG()\n",
    "        self.cove = ChainOfVerification()\n",
    "        \n",
    "        self.fallback_knowledge = {\n",
    "            'articles': \"–í–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º —è–∑—ã–∫–µ 3 —Ç–∏–ø–∞ –∞—Ä—Ç–∏–∫–ª–µ–π: –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ (le, la, les), \"\n",
    "                       \"–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ (un, une, des) –∏ —á–∞—Å—Ç–∏—á–Ω—ã–µ (du, de la, de l').\",\n",
    "            'verbs': \"–§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–µ –≥–ª–∞–≥–æ–ª—ã –¥–µ–ª—è—Ç—Å—è –Ω–∞ 3 –≥—Ä—É–ø–ø—ã: -er (1-—è), -ir —Å -issons (2-—è) \"\n",
    "                    \"–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ (3-—è). Aller - –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ 1-–π –≥—Ä—É–ø–ø—ã.\",\n",
    "            'tenses': \"–û—Å–Ω–æ–≤–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–∞: pr√©sent (–Ω–∞—Å—Ç–æ—è—â–µ–µ), pass√© compos√© (–ø—Ä–æ—à–µ–¥—à–µ–µ —Å–æ—Å—Ç–∞–≤–Ω–æ–µ), \"\n",
    "                     \"imparfait (–Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–µ), futur simple (–ø—Ä–æ—Å—Ç–æ–µ –±—É–¥—É—â–µ–µ).\"\n",
    "        }\n",
    "    \n",
    "    def evaluate_retrieval_quality(self, query: str, documents: List[str]) -> RetrievalQuality:\n",
    "        \"\"\"–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ retrieved –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\"\"\"\n",
    "        if not documents:\n",
    "            return RetrievalQuality.POOR\n",
    "        \n",
    "        qualities = []\n",
    "        for doc in documents:\n",
    "            quality, _ = self.self_rag.assess_relevance(query, doc)\n",
    "            qualities.append(quality)\n",
    "        \n",
    "        quality_order = [\n",
    "            RetrievalQuality.EXCELLENT,\n",
    "            RetrievalQuality.GOOD,\n",
    "            RetrievalQuality.PARTIAL,\n",
    "            RetrievalQuality.POOR\n",
    "        ]\n",
    "        \n",
    "        for q in quality_order:\n",
    "            if q in qualities[:3]:\n",
    "                return q\n",
    "        \n",
    "        return RetrievalQuality.POOR\n",
    "    \n",
    "    def get_correction_strategy(self, quality: RetrievalQuality) -> str:\n",
    "        \"\"\"–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞.\"\"\"\n",
    "        strategies = {\n",
    "            RetrievalQuality.EXCELLENT: \"none\",\n",
    "            RetrievalQuality.GOOD: \"supplement\",\n",
    "            RetrievalQuality.PARTIAL: \"refine\",\n",
    "            RetrievalQuality.POOR: \"fallback\",\n",
    "            RetrievalQuality.AMBIGUOUS: \"clarify\"\n",
    "        }\n",
    "        return strategies.get(quality, \"fallback\")\n",
    "    \n",
    "    def apply_correction(self, query: str, documents: List[str], strategy: str) -> Tuple[List[str], str]:\n",
    "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.\"\"\"\n",
    "        if strategy == \"none\":\n",
    "            return documents, \"\"\n",
    "        \n",
    "        elif strategy == \"supplement\":\n",
    "            note = \"–î–æ–±–∞–≤–ª–µ–Ω–∞ –±–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã –æ—Ç–≤–µ—Ç–∞.\"\n",
    "            query_lower = query.lower()\n",
    "            \n",
    "            for topic, knowledge in self.fallback_knowledge.items():\n",
    "                if topic in query_lower or any(kw in query_lower for kw in topic.split()):\n",
    "                    documents.append(f\"[–ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è] {knowledge}\")\n",
    "                    break\n",
    "            \n",
    "            return documents, note\n",
    "        \n",
    "        elif strategy == \"refine\":\n",
    "            note = \"–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–∏—Ç—å –≤–æ–ø—Ä–æ—Å.\"\n",
    "            return documents, note\n",
    "        \n",
    "        elif strategy == \"fallback\":\n",
    "            note = \"‚ö†Ô∏è –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è.\"\n",
    "            fallback_docs = [f\"[–ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è] {k}\" for k in self.fallback_knowledge.values()]\n",
    "            return fallback_docs, note\n",
    "        \n",
    "        elif strategy == \"clarify\":\n",
    "            note = \"–ù–∞–π–¥–µ–Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è. –¢—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞.\"\n",
    "            return documents, note\n",
    "        \n",
    "        return documents, \"\"\n",
    "    \n",
    "    def correct(self, query: str, documents: List[str]) -> Tuple[List[str], Dict[str, Any]]:\n",
    "        \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.\"\"\"\n",
    "        quality = self.evaluate_retrieval_quality(query, documents)\n",
    "        strategy = self.get_correction_strategy(quality)\n",
    "        corrected_docs, note = self.apply_correction(query, documents, strategy)\n",
    "        \n",
    "        metadata = {\n",
    "            'original_quality': quality.value,\n",
    "            'strategy': strategy,\n",
    "            'correction_applied': strategy != \"none\",\n",
    "            'note': note,\n",
    "            'original_doc_count': len(documents),\n",
    "            'corrected_doc_count': len(corrected_docs)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"CRAG correction: {quality.value} -> {strategy}\")\n",
    "        return corrected_docs, metadata\n",
    "\n",
    "\n",
    "print(\"‚úÖ CorrectiveRAG –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HallucinationDetector:\n",
    "    \"\"\"\n",
    "    –î–µ—Ç–µ–∫—Ç–æ—Ä –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤.\n",
    "    \n",
    "    –ú–µ—Ç–æ–¥—ã:\n",
    "    1. Lexical Grounding - –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ\n",
    "    2. Semantic Consistency - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å\n",
    "    3. Factual Verification - —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞\n",
    "    4. Confidence Calibration - –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cove = ChainOfVerification()\n",
    "        self.confidence_markers = {\n",
    "            'high': ['–≤—Å–µ–≥–¥–∞', '–Ω–∏–∫–æ–≥–¥–∞', '—Ç–æ—á–Ω–æ', '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ', '100%', '–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ'],\n",
    "            'medium': ['–æ–±—ã—á–Ω–æ', '–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ', '—á–∞—â–µ –≤—Å–µ–≥–æ', '–≤ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ'],\n",
    "            'low': ['–≤–æ–∑–º–æ–∂–Ω–æ', '–≤–µ—Ä–æ—è—Ç–Ω–æ', '–º–æ–∂–µ—Ç –±—ã—Ç—å', '–∏–Ω–æ–≥–¥–∞']\n",
    "        }\n",
    "    \n",
    "    def check_lexical_grounding(self, response: str, context: str) -> Tuple[float, List[str]]:\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç.\"\"\"\n",
    "        french_terms = re.findall(r'\\b[a-z√©√®√™√´√†√¢√§√π√ª√º√¥√∂√Æ√Ø√ß]{3,}\\b', response.lower())\n",
    "        technical_terms = re.findall(r'\\b(?:–∞—Ä—Ç–∏–∫–ª—å|–≥–ª–∞–≥–æ–ª|—Å–ø—Ä—è–∂–µ–Ω–∏–µ|–≤—Ä–µ–º—è|–Ω–∞–∫–ª–æ–Ω–µ–Ω–∏–µ)\\w*', \n",
    "                                     response.lower())\n",
    "        \n",
    "        all_terms = set(french_terms + technical_terms)\n",
    "        context_lower = context.lower()\n",
    "        \n",
    "        ungrounded = []\n",
    "        grounded_count = 0\n",
    "        \n",
    "        for term in all_terms:\n",
    "            if term in context_lower:\n",
    "                grounded_count += 1\n",
    "            elif len(term) > 4:\n",
    "                ungrounded.append(term)\n",
    "        \n",
    "        score = grounded_count / max(len(all_terms), 1)\n",
    "        return score, ungrounded[:10]\n",
    "    \n",
    "    def check_semantic_consistency(self, response: str) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞.\"\"\"\n",
    "        inconsistencies = []\n",
    "        sentences = re.split(r'[.!?]', response)\n",
    "        \n",
    "        negation_pairs = [\n",
    "            ('–≤—Å–µ–≥–¥–∞', '–Ω–∏–∫–æ–≥–¥–∞'),\n",
    "            ('–º–æ–∂–Ω–æ', '–Ω–µ–ª—å–∑—è'),\n",
    "            ('–ø—Ä–∞–≤–∏–ª—å–Ω–æ', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ'),\n",
    "            ('–º—É–∂—Å–∫–æ–π —Ä–æ–¥', '–∂–µ–Ω—Å–∫–∏–π —Ä–æ–¥'),\n",
    "        ]\n",
    "        \n",
    "        for sent1 in sentences:\n",
    "            for sent2 in sentences:\n",
    "                if sent1 == sent2:\n",
    "                    continue\n",
    "                for pos, neg in negation_pairs:\n",
    "                    if pos in sent1.lower() and neg in sent2.lower():\n",
    "                        words1 = set(sent1.lower().split())\n",
    "                        words2 = set(sent2.lower().split())\n",
    "                        if len(words1 & words2) > 3:\n",
    "                            inconsistencies.append(f\"–í–æ–∑–º–æ–∂–Ω–æ–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: {pos} vs {neg}\")\n",
    "        \n",
    "        return len(inconsistencies) == 0, inconsistencies\n",
    "    \n",
    "    def check_confidence_calibration(self, response: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ.\"\"\"\n",
    "        response_lower = response.lower()\n",
    "        overconfident = []\n",
    "        \n",
    "        for marker in self.confidence_markers['high']:\n",
    "            if marker in response_lower:\n",
    "                for sent in re.split(r'[.!?]', response):\n",
    "                    if marker in sent.lower():\n",
    "                        overconfident.append(sent.strip())\n",
    "        \n",
    "        if len(overconfident) > 2:\n",
    "            return 'overconfident', overconfident\n",
    "        elif any(m in response_lower for m in self.confidence_markers['low']):\n",
    "            return 'calibrated', []\n",
    "        else:\n",
    "            return 'neutral', []\n",
    "    \n",
    "    def detect(self, response: str, context: str) -> Dict[str, Any]:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –¥–µ—Ç–µ–∫—Ü–∏—é –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.\"\"\"\n",
    "        results = {\n",
    "            'has_hallucinations': False,\n",
    "            'confidence': 1.0,\n",
    "            'issues': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # 1. –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ\n",
    "        grounding_score, ungrounded = self.check_lexical_grounding(response, context)\n",
    "        results['grounding_score'] = grounding_score\n",
    "        results['ungrounded_terms'] = ungrounded\n",
    "        \n",
    "        if grounding_score < 0.3:\n",
    "            results['issues'].append(\"–ù–∏–∑–∫–æ–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç\")\n",
    "            results['recommendations'].append(\"–î–æ–±–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏\")\n",
    "            results['has_hallucinations'] = True\n",
    "        \n",
    "        # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å\n",
    "        is_consistent, inconsistencies = self.check_semantic_consistency(response)\n",
    "        results['is_consistent'] = is_consistent\n",
    "        results['inconsistencies'] = inconsistencies\n",
    "        \n",
    "        if not is_consistent:\n",
    "            results['issues'].append(\"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è\")\n",
    "            results['recommendations'].append(\"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π\")\n",
    "            results['has_hallucinations'] = True\n",
    "        \n",
    "        # 3. –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "        conf_level, overconfident = self.check_confidence_calibration(response)\n",
    "        results['confidence_level'] = conf_level\n",
    "        results['overconfident_claims'] = overconfident\n",
    "        \n",
    "        if conf_level == 'overconfident':\n",
    "            results['issues'].append(\"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–≤–µ—Ä—Ö—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è\")\n",
    "            results['recommendations'].append(\"–î–æ–±–∞–≤—å—Ç–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏\")\n",
    "        \n",
    "        # 4. –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "        results['confidence'] = (\n",
    "            grounding_score * 0.4 +\n",
    "            (1.0 if is_consistent else 0.5) * 0.3 +\n",
    "            (0.7 if conf_level == 'calibrated' else 1.0 if conf_level == 'neutral' else 0.5) * 0.3\n",
    "        )\n",
    "        \n",
    "        # 5. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ CoVe\n",
    "        verification = self.cove.run_verification(response, context)\n",
    "        results['verification'] = {\n",
    "            'is_verified': verification.is_verified,\n",
    "            'confidence': verification.confidence,\n",
    "            'issues': verification.issues[:3],\n",
    "            'evidence': verification.grounding_evidence[:3]\n",
    "        }\n",
    "        \n",
    "        if not verification.is_verified:\n",
    "            results['has_hallucinations'] = True\n",
    "            results['confidence'] *= 0.7\n",
    "        \n",
    "        logger.info(f\"Hallucination detection: has_hallucinations={results['has_hallucinations']}, \"\n",
    "                   f\"confidence={results['confidence']:.2f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ HallucinationDetector –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 6. FrenchAssistant - –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class FrenchAssistant:\n    \"\"\"\n    –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –Ω–µ–π—Ä–æ-—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ ‚Äî –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.\n    \n    –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:\n    - –ü–µ—Ä–µ–≤–æ–¥ —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π\n    - –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏\n    - –†–∞–±–æ—Ç–∞ —Å –∏–¥–∏–æ–º–∞–º–∏ –∏ —Ñ—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º–∞–º–∏\n    - –ü–æ–º–æ—â—å —Å —Ç–∏–ø–∏—á–Ω—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏\n    \n    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å Saiga LLaMA3 8B –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤.\n    \"\"\"\n    \n    def __init__(self, config: Dict = None, llm: SaigaLLM = None):\n        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.\"\"\"\n        logger.info(\"Initializing FrenchAssistant...\")\n        \n        self.config = config or CONFIG\n        \n        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n        self.tracer = TracingManager()\n        self.safety_filter = SafetyFilter(self.config.get('SAFETY', {}).get('input_filter', {}))\n        \n        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n        self.embeddings = self._init_embeddings()\n        \n        # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ\n        self.vectorstore = self._init_vectorstore()\n        \n        # Retriever\n        self.retriever = EnhancedRetriever(\n            self.vectorstore,\n            self.config.get('RAG_CONFIG', {}).get('retrieval', {}),\n            self.tracer\n        )\n        \n        # LLM - –∏—Å–ø–æ–ª—å–∑—É–µ–º Saiga –∏–ª–∏ fallback –Ω–∞ —à–∞–±–ª–æ–Ω—ã\n        self.llm = llm or SAIGA_LLM\n        if self.llm:\n            logger.info(\"‚úÖ LLM (Saiga) –ø–æ–¥–∫–ª—é—á–µ–Ω–∞!\")\n        else:\n            logger.warning(\"‚ö†Ô∏è LLM –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —à–∞–±–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã\")\n        \n        # –ü—Ä–æ–º–ø—Ç\n        self.prompt_template = self._create_prompt_template()\n        \n        logger.info(\"FrenchAssistant initialized successfully!\")\n    \n    def _init_embeddings(self) -> HuggingFaceEmbeddings:\n        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.\"\"\"\n        embed_config = self.config.get('VECTOR_DB', {}).get('embeddings', {})\n        model_name = embed_config.get(\n            'model', \n            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n        )\n        \n        logger.info(f\"Loading embeddings model: {model_name}\")\n        \n        return HuggingFaceEmbeddings(\n            model_name=model_name,\n            model_kwargs={'device': embed_config.get('device', 'cpu')},\n            encode_kwargs={'normalize_embeddings': True}\n        )\n    \n    def _init_vectorstore(self) -> Chroma:\n        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.\"\"\"\n        db_config = self.config.get('VECTOR_DB', {})\n        persist_dir = db_config.get('persist_directory', './data/chroma_db')\n        collection_name = db_config.get('collection_name', 'french_knowledge')\n        \n        if os.path.exists(persist_dir):\n            logger.info(f\"Loading existing vectorstore from {persist_dir}\")\n            return Chroma(\n                persist_directory=persist_dir,\n                embedding_function=self.embeddings,\n                collection_name=collection_name\n            )\n        \n        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –±–∞–∑—É —Å –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–º–∏\n        logger.info(\"Creating new vectorstore with demo data...\")\n        documents = self._create_demo_documents()\n        \n        vectorstore = Chroma.from_documents(\n            documents=documents,\n            embedding=self.embeddings,\n            persist_directory=persist_dir,\n            collection_name=collection_name\n        )\n        \n        logger.info(f\"Vectorstore created with {len(documents)} documents\")\n        return vectorstore\n    \n    def _create_demo_documents(self) -> List[Document]:\n        \"\"\"–°–æ–∑–¥–∞—ë—Ç –¥–µ–º–æ-–¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.\"\"\"\n        demo_content = [\n            # –ê—Ä—Ç–∏–∫–ª–∏\n            {\n                'content': '''# –ê—Ä—Ç–∏–∫–ª–∏ –≤–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º —è–∑—ã–∫–µ\n\n## –û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∞—Ä—Ç–∏–∫–ª–∏ (articles d√©finis)\n- le - –º—É–∂—Å–∫–æ–π —Ä–æ–¥ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞ (le livre - –∫–Ω–∏–≥–∞)\n- la - –∂–µ–Ω—Å–∫–∏–π —Ä–æ–¥ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞ (la table - —Å—Ç–æ–ª)\n- l' - –ø–µ—Ä–µ–¥ –≥–ª–∞—Å–Ω–æ–π –∏–ª–∏ h –Ω–µ–º—ã–º (l'eau - –≤–æ–¥–∞, l'homme - —á–µ–ª–æ–≤–µ–∫)\n- les - –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –æ–±–æ–∏—Ö —Ä–æ–¥–æ–≤ (les livres - –∫–Ω–∏–≥–∏)\n\n## –ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∞—Ä—Ç–∏–∫–ª–∏ (articles ind√©finis)\n- un - –º—É–∂—Å–∫–æ–π —Ä–æ–¥ (un chat - –∫–æ—Ç)\n- une - –∂–µ–Ω—Å–∫–∏–π —Ä–æ–¥ (une maison - –¥–æ–º)\n- des - –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (des fleurs - —Ü–≤–µ—Ç—ã)\n\n## –ß–∞—Å—Ç–∏—á–Ω—ã–µ –∞—Ä—Ç–∏–∫–ª–∏ (articles partitifs)\n- du - –º—É–∂—Å–∫–æ–π —Ä–æ–¥ (du pain - —Ö–ª–µ–±)\n- de la - –∂–µ–Ω—Å–∫–∏–π —Ä–æ–¥ (de la viande - –º—è—Å–æ)\n- de l' - –ø–µ—Ä–µ–¥ –≥–ª–∞—Å–Ω–æ–π (de l'eau - –≤–æ–¥—ã)\n''',\n                'metadata': {'topic': 'grammar', 'subtopic': 'articles', 'difficulty': 'beginner'}\n            },\n            # –ì–ª–∞–≥–æ–ª—ã 1-–π –≥—Ä—É–ø–ø—ã\n            {\n                'content': '''# –ì–ª–∞–≥–æ–ª—ã 1-–π –≥—Ä—É–ø–ø—ã (-er)\n\n–ì–ª–∞–≥–æ–ª—ã –Ω–∞ -er —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ–∫–æ–ª–æ 90% –≤—Å–µ—Ö —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏—Ö –≥–ª–∞–≥–æ–ª–æ–≤.\n\n## –°–ø—Ä—è–∂–µ–Ω–∏–µ –≤ pr√©sent (–Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è)\n–ü—Ä–∏–º–µ—Ä: parler (–≥–æ–≤–æ—Ä–∏—Ç—å)\n- je parle - —è –≥–æ–≤–æ—Ä—é\n- tu parles - —Ç—ã –≥–æ–≤–æ—Ä–∏—à—å\n- il/elle parle - –æ–Ω/–æ–Ω–∞ –≥–æ–≤–æ—Ä–∏—Ç\n- nous parlons - –º—ã –≥–æ–≤–æ—Ä–∏–º\n- vous parlez - –≤—ã –≥–æ–≤–æ—Ä–∏—Ç–µ\n- ils/elles parlent - –æ–Ω–∏ –≥–æ–≤–æ—Ä—è—Ç\n\n## –í–∞–∂–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è\n–ì–ª–∞–≥–æ–ª aller (–∏–¥—Ç–∏) - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –≥–ª–∞–≥–æ–ª –Ω–∞ -er, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ 3-–π –≥—Ä—É–ø–ø–µ!\n- je vais, tu vas, il va, nous allons, vous allez, ils vont\n''',\n                'metadata': {'topic': 'grammar', 'subtopic': 'verbs', 'difficulty': 'beginner'}\n            },\n            # Pass√© compos√©\n            {\n                'content': '''# Pass√© compos√© (–ø—Ä–æ—à–µ–¥—à–µ–µ —Å–æ—Å—Ç–∞–≤–Ω–æ–µ –≤—Ä–µ–º—è)\n\n## –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ\navoir/√™tre –≤ pr√©sent + participe pass√©\n\n## –° avoir (–±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≥–ª–∞–≥–æ–ª–æ–≤)\n- J'ai mang√© - –Ø –ø–æ–µ–ª\n- Tu as parl√© - –¢—ã –ø–æ–≥–æ–≤–æ—Ä–∏–ª\n- Elle a fini - –û–Ω–∞ –∑–∞–∫–æ–Ω—á–∏–ª–∞\n\n## –° √™tre (–≥–ª–∞–≥–æ–ª—ã –¥–≤–∏–∂–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—Ç–Ω—ã–µ)\n–ì–ª–∞–≥–æ–ª—ã —Å √™tre: aller, venir, arriver, partir, entrer, sortir, monter, descendre, \nna√Ætre, mourir, rester, tomber, devenir, revenir, rentrer, retourner, passer (–ø–æ)\n\n- Je suis all√©(e) - –Ø –ø–æ—à—ë–ª/–ø–æ—à–ª–∞\n- Elle est partie - –û–Ω–∞ —É—à–ª–∞\n- Nous sommes arriv√©s - –ú—ã –ø—Ä–∏–±—ã–ª–∏\n\n## –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ\n–° √™tre –ø—Ä–∏—á–∞—Å—Ç–∏–µ —Å–æ–≥–ª–∞—Å—É–µ—Ç—Å—è —Å –ø–æ–¥–ª–µ–∂–∞—â–∏–º –≤ —Ä–æ–¥–µ –∏ —á–∏—Å–ª–µ.\n''',\n                'metadata': {'topic': 'grammar', 'subtopic': 'tenses', 'difficulty': 'intermediate'}\n            },\n            # –ò–¥–∏–æ–º—ã\n            {\n                'content': '''# –§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–µ –∏–¥–∏–æ–º—ã –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è\n\n## –ü–æ–≥–æ–¥–∞ –∏ –ø—Ä–∏—Ä–æ–¥–∞\n- Il pleut des cordes - –õ—å—ë—Ç –∫–∞–∫ –∏–∑ –≤–µ–¥—Ä–∞ (–±—É–∫–≤. \"–¥–æ–∂–¥—å –≤–µ—Ä—ë–≤–∫–∞–º–∏\")\n- Avoir le cafard - –•–∞–Ω–¥—Ä–∏—Ç—å, –±—ã—Ç—å –≤ –¥–µ–ø—Ä–µ—Å—Å–∏–∏ (–±—É–∫–≤. \"–∏–º–µ—Ç—å —Ç–∞—Ä–∞–∫–∞–Ω–∞\")\n\n## –ï–¥–∞\n- Avoir la banane - –®–∏—Ä–æ–∫–æ —É–ª—ã–±–∞—Ç—å—Å—è (–±—É–∫–≤. \"–∏–º–µ—Ç—å –±–∞–Ω–∞–Ω\")\n- Raconter des salades - –í—Ä–∞—Ç—å, —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—Ç—å –Ω–µ–±—ã–ª–∏—Ü—ã (–±—É–∫–≤. \"—Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–∞–ª–∞—Ç—ã\")\n- C'est la fin des haricots - –≠—Ç–æ –∫–æ–Ω–µ—Ü! (–±—É–∫–≤. \"–∫–æ–Ω–µ—Ü —Ñ–∞—Å–æ–ª–∏\")\n\n## –ñ–∏–≤–æ—Ç–Ω—ã–µ\n- Poser un lapin - –ù–µ –ø—Ä–∏–π—Ç–∏ –Ω–∞ —Å–≤–∏–¥–∞–Ω–∏–µ (–±—É–∫–≤. \"–ø–æ—Å–∞–¥–∏—Ç—å –∫—Ä–æ–ª–∏–∫–∞\")\n- Avoir d'autres chats √† fouetter - –ò–º–µ—Ç—å –¥–µ–ª–∞ –ø–æ–≤–∞–∂–Ω–µ–µ (–±—É–∫–≤. \"–∏–º–µ—Ç—å –¥—Ä—É–≥–∏—Ö –∫–æ—à–µ–∫ –¥–ª—è –±–∏—Ç—å—è\")\n- Quand les poules auront des dents - –ö–æ–≥–¥–∞ —Ä–∞–∫ –Ω–∞ –≥–æ—Ä–µ —Å–≤–∏—Å—Ç–Ω–µ—Ç (–±—É–∫–≤. \"–∫–æ–≥–¥–∞ —É –∫—É—Ä –±—É–¥—É—Ç –∑—É–±—ã\")\n''',\n                'metadata': {'topic': 'idioms', 'subtopic': 'expressions', 'difficulty': 'intermediate'}\n            },\n            # –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏\n            {\n                'content': '''# –¢–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ–≤–æ—Ä—è—â–∏—Ö\n\n## –õ–æ–∂–Ω—ã–µ –¥—Ä—É–∑—å—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞\n- magazine ‚â† –º–∞–≥–∞–∑–∏–Ω (= –∂—É—Ä–Ω–∞–ª). –ú–∞–≥–∞–∑–∏–Ω = le magasin\n- cabinet ‚â† –∫–∞–±–∏–Ω–µ—Ç (= —à–∫–∞—Ñ, —Ç—É–∞–ª–µ—Ç). –ö–∞–±–∏–Ω–µ—Ç = le bureau\n- sympathique ‚â† —Å–∏–º–ø–∞—Ç–∏—á–Ω—ã–π (= –ø—Ä–∏—è—Ç–Ω—ã–π). –°–∏–º–ø–∞—Ç–∏—á–Ω—ã–π = joli, mignon\n- accurate ‚â† –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π (= —Ç–æ—á–Ω—ã–π). –ê–∫–∫—É—Ä–∞—Ç–Ω—ã–π = soign√©, ordonn√©\n\n## –ü—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏–µ\n- –ù–µ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç—Å—è –∫–æ–Ω–µ—á–Ω–∞—è —Å–æ–≥–ª–∞—Å–Ω–∞—è: Paris [pa Åi], –Ω–µ [pa Åis]\n- Liaison (—Å–≤—è–∑—ã–≤–∞–Ω–∏–µ): les amis [lezami]\n- –ù–æ—Å–æ–≤—ã–µ –∑–≤—É–∫–∏: on […îÃÉ], an […ëÃÉ], in […õÃÉ]\n\n## –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞\n- –î–≤–æ–π–Ω–æ–µ –æ—Ç—Ä–∏—Ü–∞–Ω–∏–µ: Je NE sais PAS (–Ω–µ —Ç–æ–ª—å–∫–æ ne, –Ω–æ –∏ pas!)\n- –ü–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ –≤ –≤–æ–ø—Ä–æ—Å–µ: Est-ce que tu viens? –∏–ª–∏ Viens-tu?\n''',\n                'metadata': {'topic': 'translation', 'subtopic': 'common_mistakes', 'difficulty': 'beginner'}\n            }\n        ]\n        \n        documents = []\n        splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=100,\n            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n        )\n        \n        for item in demo_content:\n            doc = Document(page_content=item['content'], metadata=item['metadata'])\n            chunks = splitter.split_documents([doc])\n            documents.extend(chunks)\n        \n        return documents\n    \n    def _create_prompt_template(self) -> PromptTemplate:\n        \"\"\"–°–æ–∑–¥–∞—ë—Ç —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞.\"\"\"\n        template = (\n            \"{system_prompt}\\n\\n\"\n            \"## –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\\n\"\n            \"{context}\\n\\n\"\n            \"## –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\\n\"\n            \"{question}\\n\\n\"\n            \"## –¢–≤–æ–π –æ—Ç–≤–µ—Ç:\\n\"\n        )\n        \n        return PromptTemplate(\n            input_variables=[\"system_prompt\", \"context\", \"question\"],\n            template=template\n        )\n    \n    def _generate_response(self, query: str, context: str) -> str:\n        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Saiga LLM.\"\"\"\n        # –ï—Å–ª–∏ LLM –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —à–∞–±–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç\n        if self.llm is None:\n            return self._generate_template_response(query, context)\n        \n        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è LLM\n        system_prompt = self.config.get('SYSTEM_PROMPT', '')\n        user_message = f\"\"\"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n{context}\n\n–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{query}\n\n–î–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\"\"\"\n        \n        try:\n            response = self.llm.generate(system_prompt, user_message)\n            return response\n        except Exception as e:\n            logger.error(f\"LLM generation error: {e}\")\n            return self._generate_template_response(query, context)\n    \n    def _generate_template_response(self, query: str, context: str) -> str:\n        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —à–∞–±–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (fallback –±–µ–∑ LLM).\"\"\"\n        query_lower = query.lower()\n        \n        if '–ø–µ—Ä–µ–≤–µ–¥' in query_lower or '–ø–µ—Ä–µ–≤–æ–¥' in query_lower:\n            return (\n                f\"üìù **–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø–µ—Ä–µ–≤–æ–¥**\\n\\n\"\n                f\"üí° **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**\\n\"\n                f\"{context[:600]}\\n\\n\"\n                f\"‚ö†Ô∏è **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\"\n            )\n        \n        elif '–∫–∞–∫ —Å–∫–∞–∑–∞—Ç—å' in query_lower:\n            return (\n                f\"üìù **–í–∞—à –≤–æ–ø—Ä–æ—Å:** {query}\\n\\n\"\n                f\"üìö **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**\\n\"\n                f\"{context[:600]}\\n\\n\"\n                f\"üí° **–°–æ–≤–µ—Ç:** –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ä–µ–≥–∏—Å—Ç—Ä —Ä–µ—á–∏.\"\n            )\n        \n        elif '–≥—Ä–∞–º–º–∞—Ç–∏–∫' in query_lower or '–ø—Ä–∞–≤–∏–ª' in query_lower:\n            return (\n                f\"üìñ **–ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å:** {query}\\n\\n\"\n                f\"üìö **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**\\n\"\n                f\"{context[:700]}\\n\\n\"\n                f\"üí° **–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:** –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\"\n            )\n        \n        elif '–∏–¥–∏–æ–º' in query_lower or '–≤—ã—Ä–∞–∂–µ–Ω' in query_lower:\n            return (\n                f\"üó£Ô∏è **–í–æ–ø—Ä–æ—Å –æ–± –∏–¥–∏–æ–º–∞—Ö:** {query}\\n\\n\"\n                f\"üìö **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**\\n\"\n                f\"{context[:700]}\\n\\n\"\n                f\"üí° **–°–æ–≤–µ—Ç:** –ò–¥–∏–æ–º—ã —á–∞—Å—Ç–æ –Ω–µ –ø–µ—Ä–µ–≤–æ–¥—è—Ç—Å—è –±—É–∫–≤–∞–ª—å–Ω–æ.\"\n            )\n        \n        else:\n            return (\n                f\"üìù **–í–∞—à –≤–æ–ø—Ä–æ—Å:** {query}\\n\\n\"\n                f\"üìö **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:**\\n\"\n                f\"{context[:700]}\\n\\n\"\n                f\"üí° **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –£—Ç–æ—á–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.\"\n            )\n    \n    def process_query(self, query: str) -> Dict[str, Any]:\n        \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\"\"\"\n        import time\n        start_time = time.time()\n        \n        result = {\n            'query': query,\n            'response': '',\n            'sources': [],\n            'metadata': {},\n            'trace': None,\n            'is_safe': True,\n            'error': None,\n            'used_llm': self.llm is not None\n        }\n        \n        self.tracer.log_event(\"query_received\", \"FrenchAssistant\", query, \"processing\")\n        \n        # 1. Safety check\n        is_safe, error_msg, safety_meta = self.safety_filter.filter_input(query)\n        result['metadata']['safety'] = safety_meta\n        \n        if not is_safe:\n            result['is_safe'] = False\n            result['response'] = error_msg\n            result['error'] = error_msg\n            self.tracer.log_event(\"safety_blocked\", \"SafetyFilter\", query, error_msg)\n            return result\n        \n        self.tracer.log_event(\"safety_passed\", \"SafetyFilter\", query, \"safe\")\n        \n        # 2. Retrieval\n        try:\n            docs = self.retriever.retrieve(query)\n            result['sources'] = [\n                {\n                    'content': doc.page_content[:200] + \"...\",\n                    'metadata': doc.metadata\n                }\n                for doc in docs\n            ]\n            \n            context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n            \n            self.tracer.log_event(\n                \"retrieval_complete\", \"EnhancedRetriever\",\n                query, f\"{len(docs)} documents retrieved\"\n            )\n            \n        except Exception as e:\n            logger.error(f\"Retrieval error: {e}\")\n            result['error'] = f\"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}\"\n            result['response'] = \"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\"\n            return result\n        \n        # 3. Response generation\n        try:\n            response = self._generate_response(query, context)\n            result['response'] = response\n            \n            llm_name = \"Saiga LLM\" if self.llm else \"Template\"\n            self.tracer.log_event(\n                \"response_generated\", llm_name,\n                query[:50] + \"...\", response[:100] + \"...\"\n            )\n            \n        except Exception as e:\n            logger.error(f\"Generation error: {e}\")\n            result['error'] = f\"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}\"\n            result['response'] = \"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.\"\n            return result\n        \n        # 4. Hallucination check\n        is_grounded, score, ungrounded = self.safety_filter.check_hallucination(response, context)\n        result['metadata']['grounding'] = {\n            'is_grounded': is_grounded,\n            'score': score,\n            'ungrounded_claims': ungrounded\n        }\n        \n        if not is_grounded:\n            logger.warning(f\"Potential hallucination detected: {ungrounded}\")\n            result['response'] += \"\\n\\n‚ö†Ô∏è *–ù–µ–∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏.*\"\n        \n        # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è\n        duration = (time.time() - start_time) * 1000\n        result['metadata']['total_duration_ms'] = duration\n        result['trace'] = self.tracer.get_trace_report()\n        \n        self.tracer.log_event(\n            \"query_complete\", \"FrenchAssistant\",\n            query[:50] + \"...\", f\"Response generated in {duration:.2f}ms\"\n        )\n        \n        return result\n    \n    def get_statistics(self) -> Dict:\n        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.\"\"\"\n        return {\n            'total_events': len(self.tracer.events),\n            'collection_count': self.vectorstore._collection.count(),\n            'llm_loaded': self.llm is not None,\n            'llm_model': 'IlyaGusev/saiga_llama3_8b' if self.llm else 'None (template mode)',\n            'config': {\n                'embedding_model': self.config.get('VECTOR_DB', {}).get('embeddings', {}).get('model'),\n                'chunk_size': self.config.get('RAG_CONFIG', {}).get('chunking', {}).get('chunk_size'),\n                'retrieval_k': self.config.get('RAG_CONFIG', {}).get('retrieval', {}).get('k')\n            }\n        }\n\n\nprint(\"‚úÖ FrenchAssistant –≥–æ—Ç–æ–≤ (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Saiga LLM)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 7. –î–µ–º–æ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n",
    "print(\"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è French Assistant...\")\n",
    "assistant = FrenchAssistant()\n",
    "print(\"\\n‚úÖ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "stats = assistant.get_statistics()\n",
    "print(\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç 1: –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å\n",
    "query = \"–ö–∞–∫ —Å–ø—Ä—è–≥–∞—é—Ç—Å—è –≥–ª–∞–≥–æ–ª—ã –ø–µ—Ä–≤–æ–π –≥—Ä—É–ø–ø—ã –≤–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º?\"\n",
    "result = assistant.process_query(query)\n",
    "\n",
    "print(f\"‚ùì –í–æ–ø—Ä–æ—Å: {query}\")\n",
    "print(f\"\\nü§ñ –û—Ç–≤–µ—Ç:\\n{result['response']}\")\n",
    "print(f\"\\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(result['sources'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç 2: –í–æ–ø—Ä–æ—Å –æ–± –∞—Ä—Ç–∏–∫–ª—è—Ö\n",
    "query = \"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –∞—Ä—Ç–∏–∫–ª–∏ –≤–æ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º —è–∑—ã–∫–µ\"\n",
    "result = assistant.process_query(query)\n",
    "\n",
    "print(f\"‚ùì –í–æ–ø—Ä–æ—Å: {query}\")\n",
    "print(f\"\\nü§ñ –û—Ç–≤–µ—Ç:\\n{result['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç 3: –ò–¥–∏–æ–º—ã\n",
    "query = \"–ö–∞–∫–∏–µ –µ—Å—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–µ –∏–¥–∏–æ–º—ã?\"\n",
    "result = assistant.process_query(query)\n",
    "\n",
    "print(f\"‚ùì –í–æ–ø—Ä–æ—Å: {query}\")\n",
    "print(f\"\\nü§ñ –û—Ç–≤–µ—Ç:\\n{result['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ safety filter (off-topic)\n",
    "query = \"–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è?\"\n",
    "result = assistant.process_query(query)\n",
    "\n",
    "print(f\"‚ùì –í–æ–ø—Ä–æ—Å: {query}\")\n",
    "print(f\"\\nü§ñ –û—Ç–≤–µ—Ç:\\n{result['response']}\")\n",
    "print(f\"\\nüõ°Ô∏è Safety: {result['is_safe']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\n# ‚ö†Ô∏è –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –∫–æ–¥ –Ω–∏–∂–µ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—â–µ–Ω–∏—è —Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º\n# –í —Ä–µ–∂–∏–º–µ \"Run all\" —ç—Ç–∞ —è—á–µ–π–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è\n\n\"\"\"\nprint(\"=\"*60)\nprint(\"üá´üá∑ French Assistant - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º\")\nprint(\"=\"*60)\nprint(\"–í–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞\\n\")\n\nwhile True:\n    query = input(\"üë§ –í—ã: \").strip()\n    \n    if query.lower() == 'exit':\n        print(\"\\nAu revoir! üëã\")\n        break\n    \n    if not query:\n        continue\n    \n    result = assistant.process_query(query)\n    print(f\"\\nü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:\\n{result['response']}\\n\")\n    print(\"-\"*40)\n\"\"\"\n\nprint(\"üí° –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å 'Run all'\")\nprint(\"   –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –∫–æ–¥ –≤—ã—à–µ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—â–µ–Ω–∏—è\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}