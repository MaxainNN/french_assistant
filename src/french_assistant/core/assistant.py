"""
–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å FrenchAssistant.

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RAG-—Å–∏—Å—Ç–µ–º—ã:
- Safety Filter
- Retrieval
- Generation
- Hallucination Detection
"""

import re
import time
import logging
from typing import Any, Dict, List, Optional

from langchain_core.prompts import PromptTemplate

from .config import AssistantConfig, load_config, get_openai_api_key
from ..utils.tracing import TracingManager
from ..safety.filter import SafetyFilter
from ..safety.hallucination import HallucinationDetector
from ..retrieval.vectorstore import VectorStoreManager
from ..retrieval.retriever import EnhancedRetriever

logger = logging.getLogger(__name__)


class FrenchAssistant:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –Ω–µ–π—Ä–æ-—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ ‚Äî –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.

    –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
    - –ü–µ—Ä–µ–≤–æ–¥ —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π
    - –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏
    - –†–∞–±–æ—Ç–∞ —Å –∏–¥–∏–æ–º–∞–º–∏ –∏ —Ñ—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º–∞–º–∏
    - –ü–æ–º–æ—â—å —Å —Ç–∏–ø–∏—á–Ω—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏

    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
        assistant = FrenchAssistant()
        result = assistant.process_query("–ö–∞–∫ —Å–ø—Ä—è–≥–∞–µ—Ç—Å—è –≥–ª–∞–≥–æ–ª √™tre?")
        print(result["response"])
    """

    def __init__(
        self,
        config: AssistantConfig = None,
        config_path: Optional[str] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.

        Args:
            config: –û–±—ä–µ–∫—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        logger.info("Initializing FrenchAssistant...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if config:
            self.config = config
        else:
            self.config = load_config(config_path)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.tracer = TracingManager()
        self.safety_filter = SafetyFilter(self.config.safety)
        self.hallucination_detector = HallucinationDetector(
            min_grounding_score=self.config.safety.min_grounding_score
        )

        # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ retriever
        self.vectorstore_manager = VectorStoreManager(
            config=self.config.vector_db,
            chunking_config=self.config.rag.chunking
        )
        self.retriever = EnhancedRetriever(
            vectorstore=self.vectorstore_manager.vectorstore,
            config=self.config.rag.retrieval,
            tracer=self.tracer
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
        self.llm = self._init_llm()

        # –ü—Ä–æ–º–ø—Ç
        self.prompt_template = self._create_prompt_template()

        logger.info("FrenchAssistant initialized successfully!")

    def _init_llm(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç LLM –º–æ–¥–µ–ª—å.

        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç OpenAI (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω.
        """
        if self.config.model.provider == "openai":
            api_key = get_openai_api_key()
            if not api_key:
                logger.warning(
                    "OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. "
                    "LLM –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –¥–µ–º–æ-—Ä–µ–∂–∏–º–µ —Å —à–∞–±–ª–æ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏."
                )
                return None

            try:
                from langchain_openai import ChatOpenAI

                llm = ChatOpenAI(
                    model=self.config.model.model_name,
                    temperature=self.config.model.temperature,
                    max_tokens=self.config.model.max_new_tokens,
                    api_key=api_key,
                )
                logger.info(f"OpenAI LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.config.model.model_name}")
                return llm

            except ImportError:
                logger.error(
                    "langchain-openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                    "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install langchain-openai"
                )
                return None
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI LLM: {e}")
                return None

        logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.config.model.provider}")
        return None

    def _create_prompt_template(self) -> PromptTemplate:
        """–°–æ–∑–¥–∞—ë—Ç —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞."""
        template = """
            {system_prompt}

            ## –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:
            {context}

            ## –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
            {question}

            ## –¢–≤–æ–π –æ—Ç–≤–µ—Ç:
        """
        return PromptTemplate(
            input_variables=["system_prompt", "context", "question"],
            template=template
        )

    def _generate_response(self, query: str, context: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ retrieved –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        if self.llm is None:
            return self._generate_template_response(query, context)

        prompt = self.prompt_template.format(
            system_prompt=self.config.system_prompt,
            context=context,
            question=query
        )

        response = self.llm.invoke(prompt)
        return response.content

    def _generate_template_response(self, query: str, context: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —à–∞–±–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –±–µ–∑ LLM)."""
        query_lower = query.lower()

        if "–ø–µ—Ä–µ–≤–µ–¥" in query_lower or "–ø–µ—Ä–µ–≤–æ–¥" in query_lower:
            return self._format_translation_response(query, context)
        elif "–∫–∞–∫ —Å–∫–∞–∑–∞—Ç—å" in query_lower:
            return self._format_how_to_say_response(query, context)
        elif "–≥—Ä–∞–º–º–∞—Ç–∏–∫" in query_lower or "–ø—Ä–∞–≤–∏–ª" in query_lower:
            return self._format_grammar_response(query, context)
        elif "–∏–¥–∏–æ–º" in query_lower or "–≤—ã—Ä–∞–∂–µ–Ω" in query_lower:
            return self._format_idiom_response(query, context)
        else:
            return self._format_general_response(query, context)

    def _format_translation_response(self, query: str, context: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–≤–æ–¥–∞."""
        text_match = re.search(
            r'–ø–µ—Ä–µ–≤–µ–¥[–∏|–∏—Ç–µ]?\s*:?\s*["\']?(.+?)["\']?\s*$',
            query,
            re.IGNORECASE
        )
        if text_match:
            text_to_translate = text_match.group(1)
        else:
            text_to_translate = query.replace("–ø–µ—Ä–µ–≤–µ–¥–∏", "").replace("–ø–µ—Ä–µ–≤–æ–¥", "").strip()

        return f"""üìù **–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø–µ—Ä–µ–≤–æ–¥:** "{text_to_translate}"

            üí° **–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:** –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.

            üìö **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã:**
            {context[:500]}...

            ‚ö†Ô∏è **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –î–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —É–∫–∞–∂–∏—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""

    def _format_how_to_say_response(self, query: str, context: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å '–∫–∞–∫ —Å–∫–∞–∑–∞—Ç—å'."""
        return f"""üìù **–í–∞—à –≤–æ–ø—Ä–æ—Å:** {query}

            üìö **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**
            {context[:600]}

            üí° **–°–æ–≤–µ—Ç:** –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ä–µ–≥–∏—Å—Ç—Ä —Ä–µ—á–∏."""

    def _format_grammar_response(self, query: str, context: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å."""
        return f"""üìñ **–ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å:** {query}

        üìö **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**
        {context[:700]}

        üí° **–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:** –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""

    def _format_idiom_response(self, query: str, context: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ–± –∏–¥–∏–æ–º–∞—Ö."""
        return f"""üó£Ô∏è **–í–æ–ø—Ä–æ—Å –æ–± –∏–¥–∏–æ–º–∞—Ö:** {query}

        üìö **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**
        {context[:700]}

        üí° **–°–æ–≤–µ—Ç:** –ò–¥–∏–æ–º—ã —á–∞—Å—Ç–æ –Ω–µ –ø–µ—Ä–µ–≤–æ–¥—è—Ç—Å—è –±—É–∫–≤–∞–ª—å–Ω–æ."""

    def _format_general_response(self, query: str, context: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ–±—â–∏–π –æ—Ç–≤–µ—Ç."""
        return f"""üìù **–í–∞—à –≤–æ–ø—Ä–æ—Å:** {query}

        üìö **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**
        {context[:700]}

        üí° **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –£—Ç–æ—á–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""

    def process_query(self, query: str) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        –ü–æ–ª–Ω—ã–π pipeline:
        1. Safety check
        2. Retrieval
        3. Response generation
        4. Hallucination check

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Returns:
            Dict —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏:
            - query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            - response: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            - sources: —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            - metadata: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            - is_safe: –ø—Ä–æ—à—ë–ª –ª–∏ safety check
            - error: —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        """
        start_time = time.time()

        result = {
            "query": query,
            "response": "",
            "sources": [],
            "metadata": {},
            "trace": None,
            "is_safe": True,
            "error": None
        }

        self.tracer.log_event("query_received", "FrenchAssistant", query, "processing")

        # 1. Safety check
        is_safe, error_msg, safety_meta = self.safety_filter.filter_input(query)
        result["metadata"]["safety"] = safety_meta

        if not is_safe:
            result["is_safe"] = False
            result["response"] = error_msg
            result["error"] = error_msg
            self.tracer.log_event("safety_blocked", "SafetyFilter", query, error_msg)
            return result

        self.tracer.log_event("safety_passed", "SafetyFilter", query, "safe")

        # 2. Retrieval
        try:
            docs = self.retriever.retrieve(query)
            result["sources"] = [
                {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                }
                for doc in docs
            ]
            context = "\n\n---\n\n".join([doc.page_content for doc in docs])

            self.tracer.log_event(
                "retrieval_complete",
                "EnhancedRetriever",
                query,
                f"{len(docs)} documents retrieved"
            )

        except Exception as e:
            logger.error(f"Retrieval error: {e}")
            result["error"] = f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"
            result["response"] = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
            return result

        # 3. Response generation
        try:
            response = self._generate_response(query, context)
            result["response"] = response

            self.tracer.log_event(
                "response_generated",
                "LLM",
                query[:50] + "...",
                response[:100] + "..."
            )

        except Exception as e:
            logger.error(f"Generation error: {e}")
            result["error"] = f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
            result["response"] = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
            return result

        # 4. Hallucination check
        hallucination_result = self.hallucination_detector.detect(response, context)
        result["metadata"]["grounding"] = {
            "is_grounded": not hallucination_result["has_hallucinations"],
            "score": hallucination_result["grounding_score"],
            "confidence": hallucination_result["confidence"]
        }

        if hallucination_result["has_hallucinations"]:
            logger.warning("Potential hallucination detected")
            result["response"] += "\n\n‚ö†Ô∏è *–ù–µ–∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏.*"

        # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
        duration = (time.time() - start_time) * 1000
        result["metadata"]["total_duration_ms"] = duration
        result["trace"] = self.tracer.get_trace_report()

        self.tracer.log_event(
            "query_complete",
            "FrenchAssistant",
            query[:50] + "...",
            f"Response generated in {duration:.2f}ms"
        )

        return result

    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞."""
        return {
            "total_events": len(self.tracer.events),
            "collection_count": self.vectorstore_manager.get_collection_count(),
            "config": {
                "embedding_model": self.config.vector_db.embeddings.model,
                "chunk_size": self.config.rag.chunking.chunk_size,
                "retrieval_k": self.config.rag.retrieval.k
            }
        }
